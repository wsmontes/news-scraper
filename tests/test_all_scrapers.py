"""
Teste integrado para todos os scrapers de fontes financeiras.

Valida que todos os scrapers:
1. Coletam URLs corretamente
2. Extraem metadados completos
3. Mant√™m qualidade m√≠nima (80% de sucesso)
4. Extraem datas v√°lidas
"""

import pytest
from datetime import datetime
from news_scraper.browser import BrowserConfig, ProfessionalScraper
from news_scraper.sources.pt import InfoMoneyScraper, ValorScraper, EInvestidorScraper, MoneyTimesScraper
from news_scraper.sources.en import BloombergScraper
from news_scraper.extract import extract_article_metadata


# Configura√ß√£o de todos os scrapers
SCRAPERS = [
    ("InfoMoney", InfoMoneyScraper, "infomoney.com.br"),
    ("Valor Econ√¥mico", ValorScraper, "valor.globo.com"),
    ("Bloomberg Brasil", BloombergScraper, "bloomberg.com.br"),
    ("E-Investidor", EInvestidorScraper, "einvestidor.estadao.com.br"),
    ("Money Times", MoneyTimesScraper, "moneytimes.com.br"),
]


@pytest.fixture(scope="module")
def scraper():
    """Fixture com scraper configurado."""
    config = BrowserConfig(headless=True)
    scraper = ProfessionalScraper(config)
    scraper.start()
    yield scraper
    scraper.stop()


@pytest.mark.parametrize("source_name,scraper_class,domain", SCRAPERS)
def test_all_sources_collect_urls(scraper, source_name, scraper_class, domain):
    """Testa que cada fonte consegue coletar URLs."""
    source_scraper = scraper_class(scraper)
    urls = source_scraper.get_latest_articles(limit=3)
    
    assert len(urls) > 0, f"{source_name}: Deve retornar pelo menos 1 URL"
    assert len(urls) <= 3, f"{source_name}: N√£o deve exceder o limite"
    
    for url in urls:
        assert domain in url, f"{source_name}: URL deve conter dom√≠nio {domain}"
        assert url.startswith("http"), f"{source_name}: URL deve ser v√°lida"
    
    print(f"\n‚úì {source_name}: {len(urls)} URLs coletadas")


@pytest.mark.parametrize("source_name,scraper_class,domain", SCRAPERS)
def test_all_sources_extract_metadata(scraper, source_name, scraper_class, domain):
    """Testa que cada fonte extrai metadados corretamente."""
    source_scraper = scraper_class(scraper)
    urls = source_scraper.get_latest_articles(limit=1)
    
    assert len(urls) > 0, f"{source_name}: Deve ter URLs para testar"
    
    url = urls[0]
    scraper.get_page(url, wait_time=3)
    article = extract_article_metadata(url, scraper.driver)
    
    # Valida√ß√µes essenciais
    errors = []
    
    if not article.title:
        errors.append("T√≠tulo n√£o extra√≠do")
    elif len(article.title) < 10:
        errors.append("T√≠tulo muito curto")
    
    if not article.date_published:
        errors.append("Data n√£o extra√≠da")
    elif not isinstance(article.date_published, datetime):
        errors.append("Data n√£o √© datetime")
    elif article.date_published.year < 2020:
        errors.append("Data inv√°lida")
    
    if not article.text:
        errors.append("Texto n√£o extra√≠do")
    elif len(article.text) < 100:
        errors.append("Texto muito curto")
    
    if not article.source:
        errors.append("Source n√£o identificada")
    
    if not article.scraped_at:
        errors.append("scraped_at n√£o preenchido")
    
    if errors:
        pytest.fail(f"{source_name}: {', '.join(errors)}")
    
    print(f"\n‚úì {source_name}: Metadados extra√≠dos com sucesso")
    print(f"  T√≠tulo: {article.title[:50]}...")
    print(f"  Data: {article.date_published}")
    print(f"  Autor: {article.author or 'N/A'}")
    print(f"  Texto: {len(article.text)} chars")


def test_all_sources_quality_threshold(scraper):
    """Testa que todas as fontes mant√™m qualidade m√≠nima de 80%."""
    results = []
    
    for source_name, scraper_class, domain in SCRAPERS:
        source_scraper = scraper_class(scraper)
        urls = source_scraper.get_latest_articles(limit=3)
        
        if not urls:
            continue
        
        success_count = 0
        
        for url in urls:
            try:
                scraper.get_page(url, wait_time=2)
                article = extract_article_metadata(url, scraper.driver)
                
                # Verificar se campos essenciais existem
                has_title = article.title and len(article.title) > 10
                has_date = article.date_published and isinstance(article.date_published, datetime)
                has_text = article.text and len(article.text) > 100
                
                if has_title and has_date and has_text:
                    success_count += 1
            except Exception as e:
                print(f"Erro em {url}: {e}")
                continue
        
        success_rate = success_count / len(urls) if urls else 0
        results.append((source_name, success_rate, success_count, len(urls)))
    
    print("\n" + "=" * 70)
    print("üìä RELAT√ìRIO DE QUALIDADE")
    print("=" * 70)
    
    for source_name, rate, success, total in results:
        status = "‚úÖ" if rate >= 0.8 else "‚ö†Ô∏è"
        print(f"{status} {source_name:20} {rate:>6.1%} ({success}/{total})")
    
    # Falhar se alguma fonte estiver abaixo do threshold
    failures = [name for name, rate, _, _ in results if rate < 0.8]
    
    if failures:
        pytest.fail(
            f"Fontes abaixo do threshold de 80%: {', '.join(failures)}"
        )


def test_all_sources_date_extraction(scraper):
    """Testa especificamente a extra√ß√£o de datas de todas as fontes."""
    date_results = []
    
    for source_name, scraper_class, domain in SCRAPERS:
        source_scraper = scraper_class(scraper)
        urls = source_scraper.get_latest_articles(limit=2)
        
        if not urls:
            continue
        
        dates_extracted = 0
        valid_dates = 0
        
        for url in urls:
            try:
                scraper.get_page(url, wait_time=2)
                article = extract_article_metadata(url, scraper.driver)
                
                if article.date_published:
                    dates_extracted += 1
                    
                    if (isinstance(article.date_published, datetime) and
                        2020 <= article.date_published.year <= 2030):
                        valid_dates += 1
            except Exception as e:
                print(f"Erro em {url}: {e}")
                continue
        
        date_results.append((source_name, valid_dates, dates_extracted, len(urls)))
    
    print("\n" + "=" * 70)
    print("üìÖ RELAT√ìRIO DE EXTRA√á√ÉO DE DATAS")
    print("=" * 70)
    
    for source_name, valid, extracted, total in date_results:
        rate = valid / total if total > 0 else 0
        status = "‚úÖ" if rate >= 0.8 else "‚ö†Ô∏è"
        print(f"{status} {source_name:20} {rate:>6.1%} ({valid}/{total} v√°lidas)")
    
    # Falhar se alguma fonte n√£o extrair datas adequadamente
    failures = [
        name for name, valid, extracted, total in date_results
        if (valid / total if total > 0 else 0) < 0.8
    ]
    
    if failures:
        pytest.fail(
            f"Fontes com problemas na extra√ß√£o de datas: {', '.join(failures)}"
        )


def test_compare_all_sources():
    """Teste comparativo de todas as fontes (apenas informativo)."""
    print("\n" + "=" * 70)
    print("üìã COMPARATIVO DE SCRAPERS")
    print("=" * 70)
    print(f"{'Fonte':<25} {'Dom√≠nio':<30} {'Status'}")
    print("-" * 70)
    
    for source_name, scraper_class, domain in SCRAPERS:
        print(f"{source_name:<25} {domain:<30} ‚úì")
    
    print("=" * 70)
    print(f"Total: {len(SCRAPERS)} fontes suportadas")
