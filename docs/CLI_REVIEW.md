# Revis√£o do CLI - News Scraper

## ‚úÖ O que foi implementado

### 1. **Comando `collect` - Unificado e Completo**

Novo comando principal que resolve todos os problemas identificados:

```bash
news-scraper collect --source FONTE [OPTIONS]
```

#### Funcionalidades Principais:

**‚úì M√∫ltiplas Fontes:**
- Suporte a todas as 5 fontes: InfoMoney, Money Times, Valor, Bloomberg, E-Investidor
- Op√ß√£o `--source all` para coletar de todas de uma vez
- Pode repetir `--source` para coletar de m√∫ltiplas fontes espec√≠ficas

**‚úì Filtros de Per√≠odo:**
- `--start-date YYYY-MM-DD` - Filtrar artigos por data inicial
- `--end-date YYYY-MM-DD` - Filtrar artigos por data final
- Filtragem aplicada ap√≥s coleta usando DuckDB

**‚úì Controle de Fontes:**
- `--source infomoney|moneytimes|valor|bloomberg|einvestidor|all`
- `--category` - Categoria espec√≠fica (varia por fonte)
- `--limit N` - M√°ximo de artigos por fonte

**‚úì Sistema de Proxies:**
- `--use-proxy` - Ativa sistema inteligente de proxies
- `--proxy-fallback` - Fallback autom√°tico (padr√£o: ativo)
- Proxies aprendem qual funciona melhor para cada site

**‚úì Configura√ß√µes Avan√ßadas:**
- `--headless` - Browser headless (padr√£o: ativo)
- `--delay N` - Delay entre requisi√ß√µes em segundos
- `--verbose` - Logs detalhados
- `--skip-scrape` - Apenas coletar URLs, n√£o fazer scrape
- `--urls-out FILE` - Salvar URLs em arquivo texto

**‚úì Sa√≠da:**
- `--dataset-dir` - Diret√≥rio do dataset Parquet (padr√£o: data/processed/articles)
- Particionamento autom√°tico por data

### 2. **Expans√£o do Comando `browser`**

Adicionados scrapers para todas as fontes:

```bash
news-scraper browser valor --category mercados --limit 20 --out urls.txt
news-scraper browser bloomberg --category economia --limit 15 --out urls.txt
news-scraper browser einvestidor --category mercados --limit 10 --out urls.txt
```

**Categorias por Fonte:**
- **Valor**: financas, empresas, mercados, mundo, politica, brasil
- **Bloomberg**: mercados, economia, negocios, tecnologia
- **E-Investidor**: mercados, investimentos, fundos-imobiliarios, cripto, acoes

### 3. **Documenta√ß√£o Completa**

**Arquivos Criados:**
- `docs/CLI_GUIDE.md` - Guia completo com todos os comandos
- `docs/CLI_EXAMPLES.sh` - Scripts de exemplo execut√°veis
- Exemplos pr√°ticos de workflows
- Troubleshooting e boas pr√°ticas

## üìã Exemplos de Uso

### Caso 1: Coletar InfoMoney - √∫ltimas not√≠cias

```bash
news-scraper collect --source infomoney --category mercados --limit 20
```

### Caso 2: Coletar m√∫ltiplas fontes

```bash
news-scraper collect \
  --source infomoney \
  --source moneytimes \
  --source valor \
  --limit 15
```

### Caso 3: Coletar todas as fontes

```bash
news-scraper collect --source all --limit 20
```

### Caso 4: Coletar com per√≠odo espec√≠fico

```bash
news-scraper collect \
  --source all \
  --limit 50 \
  --start-date 2026-01-01 \
  --end-date 2026-01-28
```

### Caso 5: Coletar com proxies

```bash
news-scraper collect \
  --source all \
  --limit 20 \
  --use-proxy \
  --verbose
```

### Caso 6: Apenas URLs (sem scrape)

```bash
news-scraper collect \
  --source bloomberg \
  --category mercados \
  --limit 30 \
  --skip-scrape \
  --urls-out data/raw/bloomberg_urls.txt
```

### Caso 7: Workflow completo

```bash
# Passo 1: Coletar
news-scraper collect \
  --source all \
  --category mercados \
  --limit 30 \
  --start-date 2026-01-20 \
  --end-date 2026-01-28 \
  --use-proxy \
  --delay 2.0 \
  --urls-out data/raw/urls.txt

# Passo 2: Estat√≠sticas
news-scraper stats --dataset-dir data/processed/articles

# Passo 3: Consultar
news-scraper query \
  --dataset-dir data/processed/articles \
  --sql "SELECT source, COUNT(*) as total FROM articles GROUP BY source"
```

## üéØ Todos os Par√¢metros Necess√°rios Implementados

### ‚úÖ Controle de Fonte
- [x] Escolher fonte espec√≠fica
- [x] M√∫ltiplas fontes simultaneamente
- [x] Todas as fontes de uma vez
- [x] Categorias por fonte

### ‚úÖ Controle de Per√≠odo
- [x] Data inicial
- [x] Data final
- [x] Filtragem ap√≥s coleta
- [x] Suporte a formato ISO (YYYY-MM-DD)

### ‚úÖ Configura√ß√µes de Scraping
- [x] Limite de artigos por fonte
- [x] Delay entre requisi√ß√µes
- [x] Browser headless
- [x] Modo verbose

### ‚úÖ Sistema de Proxies
- [x] Ativar/desativar proxies
- [x] Fallback autom√°tico
- [x] Aprendizado por dom√≠nio
- [x] Estat√≠sticas de sucesso

### ‚úÖ Sa√≠da e Armazenamento
- [x] Dataset Parquet particionado
- [x] Arquivo de URLs (backup)
- [x] Skip scrape (apenas URLs)
- [x] Diret√≥rio customizado

### ‚úÖ An√°lise e Consulta
- [x] SQL sobre dataset
- [x] Estat√≠sticas
- [x] Filtros por data/fonte
- [x] Export CSV/JSON

## üìä Compara√ß√£o Antes vs Depois

### ANTES (Problemas):
- ‚ùå Sem filtro de per√≠odo
- ‚ùå Sem suporte a proxies no CLI
- ‚ùå Valor, Bloomberg, E-Investidor sem CLI
- ‚ùå Sem comando unificado
- ‚ùå Sem coleta de m√∫ltiplas fontes
- ‚ùå Sem backup de URLs

### DEPOIS (Solu√ß√µes):
- ‚úÖ `--start-date` e `--end-date`
- ‚úÖ `--use-proxy` e `--proxy-fallback`
- ‚úÖ Todos os scrapers no CLI
- ‚úÖ Comando `collect` unificado
- ‚úÖ `--source` m√∫ltiplo ou `all`
- ‚úÖ `--urls-out` para backup

## üöÄ Workflows Prontos para Produ√ß√£o

### 1. Monitoramento Di√°rio

```bash
#!/bin/bash
# daily_collect.sh - Adicionar ao cron

TODAY=$(date +%Y-%m-%d)
news-scraper collect \
  --source all \
  --limit 30 \
  --dataset-dir "data/daily/$TODAY" \
  --use-proxy \
  --verbose
```

### 2. An√°lise Semanal

```bash
#!/bin/bash
# weekly_report.sh

news-scraper collect \
  --source all \
  --category mercados \
  --limit 50 \
  --start-date $(date -d "7 days ago" +%Y-%m-%d) \
  --end-date $(date +%Y-%m-%d) \
  --dataset-dir data/weekly

news-scraper stats --dataset-dir data/weekly
```

### 3. Teste e Valida√ß√£o

```bash
#!/bin/bash
# test_sources.sh

for source in infomoney moneytimes valor bloomberg einvestidor; do
  echo "Testando $source..."
  news-scraper collect \
    --source $source \
    --limit 3 \
    --skip-scrape \
    --urls-out "test_${source}.txt"
done
```

## üìù Resumo

O CLI agora oferece **controle completo** sobre:

1. **Fontes**: Todas as 5 fontes + op√ß√£o "all"
2. **Per√≠odo**: Filtros de data inicial e final
3. **Proxies**: Sistema inteligente com aprendizado
4. **Configura√ß√£o**: Delay, headless, verbose
5. **Sa√≠da**: Dataset Parquet + backup de URLs
6. **An√°lise**: SQL, stats, filtros

**Tudo que o usu√°rio precisa para:**
- Coletar not√≠cias de fontes espec√≠ficas
- Definir per√≠odo de interesse
- Usar proxies para evitar bloqueio
- Analisar dados coletados
- Automatizar coletas peri√≥dicas

## üéØ Comando Mais Comum

```bash
# Caso de uso t√≠pico: coletar √∫ltimas not√≠cias de todas as fontes
news-scraper collect --source all --limit 20 --use-proxy
```

**Pronto para produ√ß√£o!** ‚úÖ
