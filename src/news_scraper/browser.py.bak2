from __future__ import annotations

import time
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Literal, Optional

from selenium import webdriver
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

logger = logging.getLogger(__name__)


@dataclass
class BrowserConfig:
    headless: bool = True
    timeout: int = 30
    window_size: tuple[int, int] = (1920, 1080)
    user_agent: str | None = None
    implicit_wait: float = 10.0
    use_proxy: bool = False
    proxy_fallback: bool = True  # Usar fallback automático se proxy falhar


class ProfessionalScraper:
    """Scraper profissional com Selenium para sites JavaScript-heavy."""

    def __init__(self, config: BrowserConfig | None = None):
        self.config = config or BrowserConfig()
        self.driver = None
        self.current_proxy = None
        self._proxy_manager = None

    def __enter__(self):
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()

    def _get_proxy_manager(self):
        """Retorna instância do proxy manager."""
        if self._proxy_manager is None:
            from news_scraper.proxy_manager import get_proxy_manager
            self._proxy_manager = get_proxy_manager()
        return self._proxy_manager
    
    def _extract_domain(self, url: str) -> str:
        """Extrai domínio de uma URL."""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        return parsed.netloc

    def start(self, use_proxy: Optional[bool] = None) -> None:
        """
        Inicia o browser.
        
        Args:
            use_proxy: Se deve usar proxy (sobrescreve config)
        """
        if use_proxy is not None:
            self.config.use_proxy = use_proxy
        
        options = ChromeOptions()

        if self.config.headless:
            options.add_argument("--headless=new")

        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument(f"--window-size={self.config.window_size[0]},{self.config.window_size[1]}")

        if self.config.user_agent:
            options.add_argument(f"user-agent={self.config.user_agent}")
        else:
            # User agent realista
            options.add_argument(
                "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )

        # Configurar proxy se habilitado
        if self.config.use_proxy:
            proxy_manager = self._get_proxy_manager()
            self.current_proxy = proxy_manager.get_next_proxy()
            
            if self.current_proxy:
                proxy_arg = proxy_manager.get_selenium_proxy_arg(self.current_proxy)
                if proxy_arg:
                    options.add_argument(proxy_arg)
                    logger.info(f"Usando proxy: {self.current_proxy.selenium_format}"    "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )

        # Anti-detecção
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)

        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=options)
        self.driver.implicitly_wait(self.config.implicit_wait)

        # Remove webdriver property
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    def stop(self) -> None:
        """Fecha o browser."""
        if self.driver:
            self.driver.quit()
            self.driver = None
        self.current_proxy = None

    def _retry_with_proxy_fallback(self, func, *args, max_retries: int = 3, **kwargs):
        """
        Executa função com retry e fallback de proxy.
        
        Args:
            func: Função a executar
            max_retries: Número máximo de tentativas
            *args, **kwargs: Argumentos para a função
            
        Returns:
            Resultado da função
        """
        last_error = None
        domain = kwargs.pop('_domain', None)  # Domínio para estatísticas
        
        for attempt in range(max_retries):
            try:
                result = func(*args, **kwargs)
                
                # Sucesso! Marcar proxy
                if self.current_proxy and domain:
                    proxy_manager = self._get_proxy_manager()
                    proxy_manager.mark_success(self.current_proxy, domain)
                    logger.info(f"✓ Sucesso com proxy {self.current_proxy.selenium_format} em {domain}")
                
                return result
            except Exception as e:
                last_error = e
                logger.warning(f"Tentativa {attempt + 1}/{max_retries} falhou: {e}")
                
                # Se usar proxy e fallback está habilitado
                if self.config.use_proxy and self.config.proxy_fallback:
                    if self.current_proxy:
                        # Marcar proxy atual como falho
                        proxy_manager = self._get_proxy_manager()
                        proxy_manager.mark_failure(self.current_proxy, domain)
                        logger.info(f"✗ Proxy {self.current_proxy.selenium_format} falhou em {domain}")
                    
                    # Tentar próximo proxy
                    if attempt < max_retries - 1:
                        logger.info("Tentando próximo proxy...")
                        self.stop()
                        time.sleep(1)
                        
                        # Se temos domínio, usar melhor proxy para ele
                        if domain:
                            proxy_manager = self._get_proxy_manager()
                            self.current_proxy = proxy_manager.get_best_proxy_for_domain(domain)
                        
                        self.start()
        
        # Última tentativa sem proxy
        if self.config.use_proxy and self.config.proxy_fallback:
            logger.info("Tentando sem proxy...")
            self.stop()
            time.sleep(1)
            self.config.use_proxy = False
            self.start()
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.error(f"Falha mesmo sem proxy: {e}")
                raise last_error
        
        raise last_error

    def get_page(self, url: str, wait_selector: str | None = None, wait_time: int | None = None, use_retry: bool = True) -> str:
        """
        Carrega página e retorna HTML renderizado.

        Args:
            url: URL para carregar
            wait_selector: CSS selector para aguardar antes de retornar (ex.: 'article', '.content')
            wait_time: Tempo adicional de espera em segundos
            use_retry: Se deve usar retry com fallback de proxy
            
        Returns:
            HTML da página
        """
        if not self.driver:
            raise RuntimeError("Browser não iniciado. Use .start() ou context manager.")

        def _load_page():
            self.driver.get(url)

            # Aguarda elemento específico se fornecido
            if wait_selector:
                wait = WebDriverWait(self.driver, self.config.timeout)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector)))

            # Aguarda tempo adicional se fornecido
            if wait_time:
                time.sleep(wait_time)

            # Scroll para carregar lazy-load
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)

            return self.driver.page_source
        
        if use_retry and self.config.use_proxy:
            domain = self._extract_domain(url)
            return self._retry_with_proxy_fallback(_load_page, _domain=domain)
        else:
            return _load_page()

    def extract_links(
        self,
        url: str,
        selector: str,
        filter_contains: str | None = None,
        wait_time: int = 3,
    ) -> list[str]:
        """Extrai links de uma página.

        Args:
            url: URL da página
            selector: CSS selector para links (ex.: 'a[href*="/noticias/"]')
            filter_contains: Filtrar apenas URLs que contêm este texto
            wait_time: Tempo de espera após carregar
        """
        html = self.get_page(url, wait_time=wait_time)

        from bs4 import BeautifulSoup
        from urllib.parse import urljoin

        soup = BeautifulSoup(html, "lxml")
        links = soup.select(selector)

        urls: set[str] = set()
        for link in links:
            href = link.get("href")
            if not href:
                continue

            full_url = urljoin(url, href)

            if filter_contains and filter_contains not in full_url:
                continue

            urls.add(full_url)

        return sorted(urls)

    def scroll_and_load(self, scroll_pause: float = 2.0, max_scrolls: int = 5) -> str:
        """Scroll progressivo para carregar conteúdo lazy-load.

        Útil para feeds infinitos.
        """
        if not self.driver:
            raise RuntimeError("Browser não iniciado.")

        last_height = self.driver.execute_script("return document.body.scrollHeight")

        for _ in range(max_scrolls):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(scroll_pause)

            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        return self.driver.page_source


def scrape_with_browser(
    url: str,
    headless: bool = True,
    wait_selector: str | None = None,
    wait_time: int | None = None,
) -> str:
    """Helper rápido para scraping com browser."""

    config = BrowserConfig(headless=headless)

    with ProfessionalScraper(config) as scraper:
        return scraper.get_page(url, wait_selector, wait_time)
